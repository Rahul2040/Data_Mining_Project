{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH INSERT\n",
    "inserting dataset into MySQL Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        database='dmdb',\n",
    "        user='root',\n",
    "        password='password123'\n",
    "    )\n",
    "\n",
    "    if connection.is_connected():\n",
    "        db_info = connection.get_server_info()\n",
    "        print(\"Connected to MySQL Server version \", db_info)\n",
    "\n",
    "\n",
    "        df = pd.read_csv(\"D:/Data Mining/project/dataset/archive/detailed_reviews.csv\")\n",
    "        df1=df[0:10]\n",
    "\n",
    "        engine = create_engine(\"mysql+mysqlconnector://root:password123@localhost/dmdb\")\n",
    " \n",
    "        df1.to_sql(name='detailed_reviews_1', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "        print(\"Data inserted into MySQL table successfully!\")\n",
    "\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"Error connecting to MySQL\", e)\n",
    "\n",
    "finally:\n",
    "    # Close database connection\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Non English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# stemming tool from nltk\n",
    "stemmer = PorterStemmer()\n",
    "# a mapping dictionary that help remove punctuations\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "def get_tokens(df_col):\n",
    "  text=df_col['comments_without_br']\n",
    "  # turn document into lowercase\n",
    "  lowers = text.lower()\n",
    "  # remove punctuations\n",
    "  no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "  # tokenize document\n",
    "  tokens = nltk.word_tokenize(no_punctuation)\n",
    "  # remove stop words\n",
    "  filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "  # stemming process\n",
    "  stemmed = []\n",
    "  for item in filtered:\n",
    "      stemmed.append(stemmer.stem(item))\n",
    "  # final unigrams\n",
    "  return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...: 100%|██████████| 875419/875419 [1:43:56<00:00, 140.36it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:password123@localhost/dmdb\")\n",
    "\n",
    "query = \"SELECT * FROM comments_in_english\"\n",
    "df = pd.read_sql(query, engine)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Processing...\")\n",
    "\n",
    "df['comments_without_br'] = df.progress_apply(get_tokens,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...: 100%|██████████| 875419/875419 [00:04<00:00, 210041.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def joiner(df1):\n",
    "    return \" \".join(df1['comments_without_br'])\n",
    "\n",
    "df['comments_without_br']=df.progress_apply(joiner,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   listing_id       id        date  reviewer_id reviewer_name  \\\n",
      "0        2595    19760  2009-12-10        38960         Anita   \n",
      "1        2595    34320  2010-04-09        71130       Kai-Uwe   \n",
      "2        2595    46312  2010-05-25       117113        Alicia   \n",
      "3        2595  1238204  2012-05-07      1783688        Sergey   \n",
      "4        2595  1293632  2012-05-17      1870771          Loïc   \n",
      "\n",
      "                                            comments  \\\n",
      "0  I've stayed with my friend at the Midtown Cast...   \n",
      "1  We've been staying here for about 9 nights, en...   \n",
      "2  We had a wonderful stay at Jennifer's charming...   \n",
      "3  Hi to everyone!\\r<br/>Would say our greatest c...   \n",
      "4  Jennifer was very friendly and helpful, and he...   \n",
      "\n",
      "                                 comments_without_br  \n",
      "0  ive stay friend midtown castl six day love pla...  \n",
      "1  weve stay 9 night enjoy center citi never slee...  \n",
      "2  wonder stay jennif charm apart organ help woul...  \n",
      "3  hi everyon would say greatest compliment jenni...  \n",
      "4  jennif friendli help place exactli advertis lo...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_mysql(df, table_name, connection, chunksize=1000):\n",
    "    # cursor = connection.cursor()\n",
    "    for i in range(0, len(df), chunksize):\n",
    "        chunk = df[i:i+chunksize]\n",
    "        chunk.to_sql(name=table_name, con=connection, if_exists='append', index=False)\n",
    "        print(f\"\\rInserted {(i+chunksize)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 rows\n",
      "Inserted 20000 rows\n",
      "Inserted 30000 rows\n",
      "Inserted 40000 rows\n",
      "Inserted 50000 rows\n",
      "Inserted 60000 rows\n",
      "Inserted 70000 rows\n",
      "Inserted 80000 rows\n",
      "Inserted 90000 rows\n",
      "Inserted 100000 rows\n",
      "Inserted 110000 rows\n",
      "Inserted 120000 rows\n",
      "Inserted 130000 rows\n",
      "Inserted 140000 rows\n",
      "Inserted 150000 rows\n",
      "Inserted 160000 rows\n",
      "Inserted 170000 rows\n",
      "Inserted 180000 rows\n",
      "Inserted 190000 rows\n",
      "Inserted 200000 rows\n",
      "Inserted 210000 rows\n",
      "Inserted 220000 rows\n",
      "Inserted 230000 rows\n",
      "Inserted 240000 rows\n",
      "Inserted 250000 rows\n",
      "Inserted 260000 rows\n",
      "Inserted 270000 rows\n",
      "Inserted 280000 rows\n",
      "Inserted 290000 rows\n",
      "Inserted 300000 rows\n",
      "Inserted 310000 rows\n",
      "Inserted 320000 rows\n",
      "Inserted 330000 rows\n",
      "Inserted 340000 rows\n",
      "Inserted 350000 rows\n",
      "Inserted 360000 rows\n",
      "Inserted 370000 rows\n",
      "Inserted 380000 rows\n",
      "Inserted 390000 rows\n",
      "Inserted 400000 rows\n",
      "Inserted 410000 rows\n",
      "Inserted 420000 rows\n",
      "Inserted 430000 rows\n",
      "Inserted 440000 rows\n",
      "Inserted 450000 rows\n",
      "Inserted 460000 rows\n",
      "Inserted 470000 rows\n",
      "Inserted 480000 rows\n",
      "Inserted 490000 rows\n",
      "Inserted 500000 rows\n",
      "Inserted 510000 rows\n",
      "Inserted 520000 rows\n",
      "Inserted 530000 rows\n",
      "Inserted 540000 rows\n",
      "Inserted 550000 rows\n",
      "Inserted 560000 rows\n",
      "Inserted 570000 rows\n",
      "Inserted 580000 rows\n",
      "Inserted 590000 rows\n",
      "Inserted 600000 rows\n",
      "Inserted 610000 rows\n",
      "Inserted 620000 rows\n",
      "Inserted 630000 rows\n",
      "Inserted 640000 rows\n",
      "Inserted 650000 rows\n",
      "Inserted 660000 rows\n",
      "Inserted 670000 rows\n",
      "Inserted 680000 rows\n",
      "Inserted 690000 rows\n",
      "Inserted 700000 rows\n",
      "Inserted 710000 rows\n",
      "Inserted 720000 rows\n",
      "Inserted 730000 rows\n",
      "Inserted 740000 rows\n",
      "Inserted 750000 rows\n",
      "Inserted 760000 rows\n",
      "Inserted 770000 rows\n",
      "Inserted 780000 rows\n",
      "Inserted 790000 rows\n",
      "Inserted 800000 rows\n",
      "Inserted 810000 rows\n",
      "Inserted 820000 rows\n",
      "Inserted 830000 rows\n",
      "Inserted 840000 rows\n",
      "Inserted 850000 rows\n",
      "Inserted 860000 rows\n",
      "Inserted 870000 rows\n",
      "Inserted 880000 rows\n"
     ]
    }
   ],
   "source": [
    "def insert_into_mysql(df, table_name, connection, chunksize=1000):\n",
    "    # cursor = connection.cursor()\n",
    "    for i in range(0, len(df), chunksize):\n",
    "        chunk = df[i:i+chunksize]\n",
    "        chunk.to_sql(name=table_name, con=connection, if_exists='append', index=False)\n",
    "        print(f\"\\rInserted {(i+chunksize)} rows\")\n",
    "\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:password123@localhost/dmdb\")\n",
    "insert_into_mysql(df, \"nltk_filtered\", engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Tokenize the reviews\n",
    "nltk.download('punkt')  # Download the tokenizer model\n",
    "df['Tokens'] = df['comments_without_br'].progress_apply(word_tokenize)\n",
    "\n",
    "# Define keywords for good, bad, and neutral sentiments\n",
    "good_words = ['excellent', 'perfectly', 'good','great','lovely','convenient','wonderful','helpful','comfortable','awesome','happy',\n",
    "              'spacious','positive','super','superb','nice','fantastic','best','satisfied','love','recommend','like','definitely',\n",
    "              'impressive','topnotch','outstanding','welcome','grateful','trustworthy']\n",
    "bad_words = ['terrible', 'worst', 'waste', 'not', 'bad','unclean','dirty','old','negative','disappoint','waste','horrible','awful',\n",
    "             'poor','disgusting','regret','miserable','crappy','disaster','displeased','dislike','unsatisfied','unwelcome','dissatisfaction',\n",
    "             'upset','annoying']\n",
    "\n",
    "good_words_stem = [stemmer.stem(word) for word in good_words]\n",
    "good_words=good_words_stem\n",
    "\n",
    "\n",
    "bad_words_stem = [stemmer.stem(word) for word in bad_words]\n",
    "bad_words=bad_words_stem\n",
    "\n",
    "# Function to classify sentiment based on keywords\n",
    "def classify_sentiment(tokens):\n",
    "    good_count = sum(1 for word in tokens if word.lower() in good_words)\n",
    "    bad_count = sum(1 for word in tokens if word.lower() in bad_words)\n",
    "    bad_count*= 2\n",
    "\n",
    "    if good_count > bad_count:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Apply sentiment classification to each review\n",
    "df['Sentiment'] = df['Tokens'].progress_apply(classify_sentiment)\n",
    "df.drop(columns=['Tokens'],inplace=True)\n",
    "\n",
    "insert_into_mysql(df, \"nltk_filtered_with_sentiment\", engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terribl', 'worst', 'wast', 'not', 'bad', 'unclean', 'dirti', 'old', 'neg', 'disappoint', 'wast', 'horribl', 'aw', 'poor', 'disgust', 'regret', 'miser', 'crappi', 'disast', 'displeas', 'dislik', 'unsatisfi', 'unwelcom', 'dissatisfact', 'upset', 'annoy']\n"
     ]
    }
   ],
   "source": [
    "print(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_mysql(df, \"nltk_filtered_with_sentiment\", engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer,PatternAnalyzer\n",
    "\n",
    "\n",
    "def getTextBlobSentiment(df1):\n",
    "    text=df1['comments']\n",
    "    ploarity=TextBlob(text,analyzer=PatternAnalyzer()).sentiment.polarity\n",
    "    if(ploarity>0):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# a=TextBlob(mytext,analyzer=PatternAnalyzer()).sentiment.polarity\n",
    "df['Sentiment_from_text_blob']=df.progress_apply(getTextBlobSentiment,axis=1)\n",
    "insert_into_mysql(df, \"nltk_filtered_with_sentiment_text_blob\", engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing...: 100%|██████████| 875419/875419 [02:11<00:00, 6644.13it/s]\n",
      "Processing...: 100%|██████████| 875419/875419 [00:18<00:00, 46397.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 rows\n",
      "Inserted 20000 rows\n",
      "Inserted 30000 rows\n",
      "Inserted 40000 rows\n",
      "Inserted 50000 rows\n",
      "Inserted 60000 rows\n",
      "Inserted 70000 rows\n",
      "Inserted 80000 rows\n",
      "Inserted 90000 rows\n",
      "Inserted 100000 rows\n",
      "Inserted 110000 rows\n",
      "Inserted 120000 rows\n",
      "Inserted 130000 rows\n",
      "Inserted 140000 rows\n",
      "Inserted 150000 rows\n",
      "Inserted 160000 rows\n",
      "Inserted 170000 rows\n",
      "Inserted 180000 rows\n",
      "Inserted 190000 rows\n",
      "Inserted 200000 rows\n",
      "Inserted 210000 rows\n",
      "Inserted 220000 rows\n",
      "Inserted 230000 rows\n",
      "Inserted 240000 rows\n",
      "Inserted 250000 rows\n",
      "Inserted 260000 rows\n",
      "Inserted 270000 rows\n",
      "Inserted 280000 rows\n",
      "Inserted 290000 rows\n",
      "Inserted 300000 rows\n",
      "Inserted 310000 rows\n",
      "Inserted 320000 rows\n",
      "Inserted 330000 rows\n",
      "Inserted 340000 rows\n",
      "Inserted 350000 rows\n",
      "Inserted 360000 rows\n",
      "Inserted 370000 rows\n",
      "Inserted 380000 rows\n",
      "Inserted 390000 rows\n",
      "Inserted 400000 rows\n",
      "Inserted 410000 rows\n",
      "Inserted 420000 rows\n",
      "Inserted 430000 rows\n",
      "Inserted 440000 rows\n",
      "Inserted 450000 rows\n",
      "Inserted 460000 rows\n",
      "Inserted 470000 rows\n",
      "Inserted 480000 rows\n",
      "Inserted 490000 rows\n",
      "Inserted 500000 rows\n",
      "Inserted 510000 rows\n",
      "Inserted 520000 rows\n",
      "Inserted 530000 rows\n",
      "Inserted 540000 rows\n",
      "Inserted 550000 rows\n",
      "Inserted 560000 rows\n",
      "Inserted 570000 rows\n",
      "Inserted 580000 rows\n",
      "Inserted 590000 rows\n",
      "Inserted 600000 rows\n",
      "Inserted 610000 rows\n",
      "Inserted 620000 rows\n",
      "Inserted 630000 rows\n",
      "Inserted 640000 rows\n",
      "Inserted 650000 rows\n",
      "Inserted 660000 rows\n",
      "Inserted 670000 rows\n",
      "Inserted 680000 rows\n",
      "Inserted 690000 rows\n",
      "Inserted 700000 rows\n",
      "Inserted 710000 rows\n",
      "Inserted 720000 rows\n",
      "Inserted 730000 rows\n",
      "Inserted 740000 rows\n",
      "Inserted 750000 rows\n",
      "Inserted 760000 rows\n",
      "Inserted 770000 rows\n",
      "Inserted 780000 rows\n",
      "Inserted 790000 rows\n",
      "Inserted 800000 rows\n",
      "Inserted 810000 rows\n",
      "Inserted 820000 rows\n",
      "Inserted 830000 rows\n",
      "Inserted 840000 rows\n",
      "Inserted 850000 rows\n",
      "Inserted 860000 rows\n",
      "Inserted 870000 rows\n",
      "Inserted 880000 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from  sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Processing...\")\n",
    "\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:password123@localhost/dmdb\")\n",
    "\n",
    "query = \"SELECT * FROM nltk_filtered_with_sentiment\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "print(\"df created\")\n",
    "stemmer = PorterStemmer()\n",
    "# Tokenize the reviews\n",
    "\n",
    "nltk.download('punkt')  # Download the tokenizer model\n",
    "df['Tokens'] = df['comments_without_br'].progress_apply(word_tokenize)\n",
    "\n",
    "# Define keywords for good, bad, and neutral sentiments\n",
    "good_words = ['excellent', 'perfectly', 'good','great','lovely','convenient','wonderful','helpful','comfortable','awesome','happy',\n",
    "              'spacious','positive','super','superb','nice','fantastic','best','satisfied','love','recommend','like','definitely',\n",
    "              'impressive','topnotch','outstanding','welcome','grateful','trustworthy']\n",
    "bad_words = ['terrible', 'worst', 'waste', 'not', 'bad','unclean','dirty','negative','disappoint','waste','horrible','awful',\n",
    "             'poor','disgusting','regret','miserable','crappy','disaster','displeased','dislike','unsatisfied','unwelcome','dissatisfaction',\n",
    "             'upset','annoying']\n",
    "\n",
    "good_words_stem = [stemmer.stem(word) for word in good_words]\n",
    "good_words=good_words_stem\n",
    "\n",
    "\n",
    "bad_words_stem = [stemmer.stem(word) for word in bad_words]\n",
    "bad_words=bad_words_stem\n",
    "\n",
    "# Function to classify sentiment based on keywords\n",
    "def classify_sentiment(tokens):\n",
    "    good_count = sum(1 for word in tokens if word.lower() in good_words)\n",
    "    bad_count = sum(1 for word in tokens if word.lower() in bad_words)\n",
    "    bad_count*= 2\n",
    "    if good_count ==0 and bad_count==0:\n",
    "        return 0\n",
    "    if good_count > bad_count:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Apply sentiment classification to each review\n",
    "df['Sentiment'] = df['Tokens'].progress_apply(classify_sentiment)\n",
    "df.drop(columns=['Tokens'],inplace=True)\n",
    "\n",
    "insert_into_mysql(df, \"nltk_filtered_3_class\", engine, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 rows\n",
      "Inserted 20000 rows\n",
      "Inserted 30000 rows\n",
      "Inserted 40000 rows\n",
      "Inserted 50000 rows\n",
      "Inserted 60000 rows\n",
      "Inserted 70000 rows\n",
      "Inserted 80000 rows\n",
      "Inserted 90000 rows\n",
      "Inserted 100000 rows\n",
      "Inserted 110000 rows\n",
      "Inserted 120000 rows\n",
      "Inserted 130000 rows\n",
      "Inserted 140000 rows\n",
      "Inserted 150000 rows\n",
      "Inserted 160000 rows\n",
      "Inserted 170000 rows\n",
      "Inserted 180000 rows\n",
      "Inserted 190000 rows\n",
      "Inserted 200000 rows\n",
      "Inserted 210000 rows\n",
      "Inserted 220000 rows\n",
      "Inserted 230000 rows\n",
      "Inserted 240000 rows\n",
      "Inserted 250000 rows\n",
      "Inserted 260000 rows\n",
      "Inserted 270000 rows\n",
      "Inserted 280000 rows\n",
      "Inserted 290000 rows\n",
      "Inserted 300000 rows\n",
      "Inserted 310000 rows\n",
      "Inserted 320000 rows\n",
      "Inserted 330000 rows\n",
      "Inserted 340000 rows\n",
      "Inserted 350000 rows\n",
      "Inserted 360000 rows\n",
      "Inserted 370000 rows\n",
      "Inserted 380000 rows\n",
      "Inserted 390000 rows\n",
      "Inserted 400000 rows\n",
      "Inserted 410000 rows\n",
      "Inserted 420000 rows\n",
      "Inserted 430000 rows\n",
      "Inserted 440000 rows\n",
      "Inserted 450000 rows\n",
      "Inserted 460000 rows\n",
      "Inserted 470000 rows\n",
      "Inserted 480000 rows\n",
      "Inserted 490000 rows\n",
      "Inserted 500000 rows\n",
      "Inserted 510000 rows\n",
      "Inserted 520000 rows\n",
      "Inserted 530000 rows\n",
      "Inserted 540000 rows\n",
      "Inserted 550000 rows\n",
      "Inserted 560000 rows\n",
      "Inserted 570000 rows\n",
      "Inserted 580000 rows\n",
      "Inserted 590000 rows\n",
      "Inserted 600000 rows\n",
      "Inserted 610000 rows\n",
      "Inserted 620000 rows\n",
      "Inserted 630000 rows\n",
      "Inserted 640000 rows\n",
      "Inserted 650000 rows\n",
      "Inserted 660000 rows\n",
      "Inserted 670000 rows\n",
      "Inserted 680000 rows\n",
      "Inserted 690000 rows\n",
      "Inserted 700000 rows\n",
      "Inserted 710000 rows\n",
      "Inserted 720000 rows\n",
      "Inserted 730000 rows\n",
      "Inserted 740000 rows\n",
      "Inserted 750000 rows\n",
      "Inserted 760000 rows\n",
      "Inserted 770000 rows\n",
      "Inserted 780000 rows\n",
      "Inserted 790000 rows\n",
      "Inserted 800000 rows\n",
      "Inserted 810000 rows\n",
      "Inserted 820000 rows\n",
      "Inserted 830000 rows\n",
      "Inserted 840000 rows\n",
      "Inserted 850000 rows\n",
      "Inserted 860000 rows\n",
      "Inserted 870000 rows\n",
      "Inserted 880000 rows\n"
     ]
    }
   ],
   "source": [
    "insert_into_mysql(df, \"nltk_filtered_3_class\", engine, chunksize=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
